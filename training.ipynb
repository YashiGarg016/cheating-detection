{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44e0ffe-25bf-4bd4-89f4-6da4093f3d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/c6/99/ed35197a158f1fdc2fe7c3680e9c70d0128f662e1fee4ed495f4b5e13db0/scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\yashi garg\\mp-env\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\yashi garg\\mp-env\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yashi garg\\mp-env\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB 640.0 kB/s eta 0:00:14\n",
      "   ---------------------------------------- 0.1/8.7 MB 648.1 kB/s eta 0:00:14\n",
      "    --------------------------------------- 0.2/8.7 MB 1.7 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/8.7 MB 2.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/8.7 MB 3.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/8.7 MB 3.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.9/8.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.1/8.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/8.7 MB 3.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.5/8.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.6/8.7 MB 3.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.7/8.7 MB 3.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.0/8.7 MB 3.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/8.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/8.7 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.4/8.7 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.6/8.7 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.0/8.7 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.4/8.7 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.8/8.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.0/8.7 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.4/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.7/8.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.8/8.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.0/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.3/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.4/8.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.6/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.7/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.9/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.2/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.4/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.6/8.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.0/8.7 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.3/8.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.7/8.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.0/8.7 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.4/8.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 5.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.7.2 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0d811e-8148-4035-9123-eefe80e5926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 0 for FOCUSED, 1 for DISTRACTED, ESC to quit and train model.\n",
      "Model trained and saved as gaze_model.pkl!\n",
      "Press 0 for FOCUSED, 1 for DISTRACTED, ESC to quit and train model.\n",
      "Model trained and saved as gaze_model.pkl!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# -------------------- Initialize models --------------------\n",
    "PHONE_CLASSES = [\"cell phone\", \"mobile phone\"]\n",
    "phone_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "POSE_LANDMARKS = [1, 33, 263, 61, 291, 199]\n",
    "LEFT_EYE = [33, 133, 159, 145]\n",
    "LEFT_IRIS = [468, 469, 470, 471]\n",
    "RIGHT_EYE = [362, 263, 386, 374]\n",
    "RIGHT_IRIS = [473, 474, 475, 476]\n",
    "\n",
    "# -------------------- Smoothers --------------------\n",
    "class TwoDimSmoother:\n",
    "    def __init__(self, alpha=0.3):\n",
    "        self.alpha = alpha\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def apply(self, newx, newy):\n",
    "        if self.x is None:\n",
    "            self.x, self.y = newx, newy\n",
    "        else:\n",
    "            self.x = (1 - self.alpha) * self.x + self.alpha * newx\n",
    "            self.y = (1 - self.alpha) * self.y + self.alpha * newy\n",
    "        return int(self.x), int(self.y)\n",
    "\n",
    "class HeadPoseSmoother:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.pitch = None\n",
    "        self.yaw = None\n",
    "        self.roll = None\n",
    "    def apply(self, pitch, yaw, roll):\n",
    "        if self.pitch is None:\n",
    "            self.pitch, self.yaw, self.roll = pitch, yaw, roll\n",
    "        else:\n",
    "            self.pitch = (1 - self.alpha) * self.pitch + self.alpha * pitch\n",
    "            self.yaw   = (1 - self.alpha) * self.yaw   + self.alpha * yaw\n",
    "            self.roll  = (1 - self.alpha) * self.roll  + self.alpha * roll\n",
    "        return self.pitch, self.yaw, self.roll\n",
    "\n",
    "kalman_left = TwoDimSmoother()\n",
    "kalman_right = TwoDimSmoother()\n",
    "head_smoother = HeadPoseSmoother()\n",
    "\n",
    "# -------------------- Eye direction --------------------\n",
    "def get_eye_direction(landmarks, eye_idx, iris_idx, w, h, smoother):\n",
    "    eye = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in eye_idx]\n",
    "    iris_pts = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in iris_idx]\n",
    "    iris_cx = sum([p[0] for p in iris_pts]) / len(iris_pts)\n",
    "    iris_cy = sum([p[1] for p in iris_pts]) / len(iris_pts)\n",
    "    iris_x, iris_y = smoother.apply(iris_cx, iris_cy)\n",
    "\n",
    "    left, right = min(eye[0][0], eye[1][0]), max(eye[0][0], eye[1][0])\n",
    "    top, bottom = min(eye[2][1], eye[3][1]), max(eye[2][1], eye[3][1])\n",
    "\n",
    "    margin_x = int((right - left) * 0.15)\n",
    "    margin_y = int((bottom - top) * 0.20)\n",
    "\n",
    "    # Determine gaze direction and offsets\n",
    "    if iris_x < left + margin_x:\n",
    "        return \"LEFT\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    elif iris_x > right - margin_x:\n",
    "        return \"RIGHT\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    elif iris_y < top + margin_y:\n",
    "        return \"UP\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    elif iris_y > bottom - margin_y:\n",
    "        return \"DOWN\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    else:\n",
    "        return \"CENTER\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "\n",
    "# -------------------- Head pose --------------------\n",
    "def estimate_head_pose(landmarks, w, h):\n",
    "    image_points = np.array([(landmarks[idx].x * w, landmarks[idx].y * h) for idx in POSE_LANDMARKS], dtype=\"double\")\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),\n",
    "        (-30.0, 0.0, -30.0),\n",
    "        (30.0, 0.0, -30.0),\n",
    "        (-30.0, 0.0, -90.0),\n",
    "        (30.0, 0.0, -90.0),\n",
    "        (0.0, 40.0, -50.0)\n",
    "    ])\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    camera_matrix = np.array([[focal_length,0,center[0]],[0,focal_length,center[1]],[0,0,1]])\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "    success, rotation_vector, _ = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    return rotation_vector if success else None\n",
    "\n",
    "# -------------------- Collect data --------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "dir_map = {\"LEFT\":0,\"RIGHT\":1,\"UP\":2,\"DOWN\":3,\"CENTER\":4}\n",
    "\n",
    "columns = [\"left_dir\",\"right_dir\",\"left_dx\",\"left_dy\",\"right_dx\",\"right_dy\",\n",
    "           \"pitch\",\"yaw\",\"roll\",\"phone_detected\",\"eyes_away_duration\",\"label\"]\n",
    "data_rows = []\n",
    "\n",
    "away_start_time = None\n",
    "AWAY_THRESHOLD = 0.8\n",
    "\n",
    "print(\"Press 0 for FOCUSED, 1 for DISTRACTED, ESC to quit and train model.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    frame = cv2.flip(frame,1)\n",
    "    h,w,_ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    eyes_away = False\n",
    "    phone_detected = 0\n",
    "\n",
    "    # Face and eyes\n",
    "    face_results = face_mesh.process(rgb)\n",
    "    left_dir = right_dir = \"CENTER\"\n",
    "    left_dx = left_dy = right_dx = right_dy = 0\n",
    "    pitch = yaw = roll = 0\n",
    "    eyes_away_duration = 0\n",
    "\n",
    "    if face_results.multi_face_landmarks:\n",
    "        landmarks = face_results.multi_face_landmarks[0].landmark\n",
    "        left_dir, left_dx, left_dy = get_eye_direction(landmarks, LEFT_EYE, LEFT_IRIS, w, h, kalman_left)\n",
    "        right_dir, right_dx, right_dy = get_eye_direction(landmarks, RIGHT_EYE, RIGHT_IRIS, w, h, kalman_right)\n",
    "        rot_vec = estimate_head_pose(landmarks, w, h)\n",
    "        if rot_vec is not None:\n",
    "            pitch, yaw, roll = head_smoother.apply(*rot_vec.ravel())\n",
    "        if left_dir!=\"CENTER\" or right_dir!=\"CENTER\":\n",
    "            eyes_away = True\n",
    "\n",
    "    # Phone detection\n",
    "    results = phone_model(frame, verbose=False)\n",
    "    for box in results[0].boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        label = phone_model.names[cls]\n",
    "        conf = float(box.conf[0])\n",
    "        if label in PHONE_CLASSES and conf>0.5:\n",
    "            phone_detected = 1\n",
    "\n",
    "    # Eye-away timer\n",
    "    current_time = time.time()\n",
    "    if eyes_away:\n",
    "        if away_start_time is None: away_start_time=current_time\n",
    "        eyes_away_duration = current_time - away_start_time\n",
    "    else:\n",
    "        away_start_time=None\n",
    "        eyes_away_duration=0\n",
    "\n",
    "    # Display frame\n",
    "    cv2.putText(frame,\"Press 0:FOCUSED 1:DISTRACTED\",(10,30),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,255,255),2)\n",
    "    cv2.imshow(\"Data Collection\", frame)\n",
    "\n",
    "    # Wait for key\n",
    "    key = cv2.waitKey(1)\n",
    "    if key==27: break  # ESC to quit\n",
    "    label = None\n",
    "    if key==ord(\"0\"): label=0\n",
    "    elif key==ord(\"1\"): label=1\n",
    "    if label is not None:\n",
    "        row = [dir_map[left_dir], dir_map[right_dir], left_dx, left_dy, right_dx, right_dy,\n",
    "               pitch, yaw, roll, phone_detected, eyes_away_duration, label]\n",
    "        data_rows.append(row)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# -------------------- Train model --------------------\n",
    "df = pd.DataFrame(data_rows, columns=columns)\n",
    "X = df.drop(\"label\", axis=1)\n",
    "y = df[\"label\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X, y)\n",
    "\n",
    "joblib.dump(clf, \"gaze_model.pkl\")\n",
    "print(\"Model trained and saved as gaze_model.pkl!\")\n",
    "\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# -------------------- Initialize models --------------------\n",
    "PHONE_CLASSES = [\"cell phone\", \"mobile phone\"]\n",
    "phone_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "POSE_LANDMARKS = [1, 33, 263, 61, 291, 199]\n",
    "LEFT_EYE = [33, 133, 159, 145]\n",
    "LEFT_IRIS = [468, 469, 470, 471]\n",
    "RIGHT_EYE = [362, 263, 386, 374]\n",
    "RIGHT_IRIS = [473, 474, 475, 476]\n",
    "\n",
    "# -------------------- Smoothers --------------------\n",
    "class TwoDimSmoother:\n",
    "    def __init__(self, alpha=0.3):\n",
    "        self.alpha = alpha\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def apply(self, newx, newy):\n",
    "        if self.x is None:\n",
    "            self.x, self.y = newx, newy\n",
    "        else:\n",
    "            self.x = (1 - self.alpha) * self.x + self.alpha * newx\n",
    "            self.y = (1 - self.alpha) * self.y + self.alpha * newy\n",
    "        return int(self.x), int(self.y)\n",
    "\n",
    "class HeadPoseSmoother:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.pitch = None\n",
    "        self.yaw = None\n",
    "        self.roll = None\n",
    "    def apply(self, pitch, yaw, roll):\n",
    "        if self.pitch is None:\n",
    "            self.pitch, self.yaw, self.roll = pitch, yaw, roll\n",
    "        else:\n",
    "            self.pitch = (1 - self.alpha) * self.pitch + self.alpha * pitch\n",
    "            self.yaw   = (1 - self.alpha) * self.yaw   + self.alpha * yaw\n",
    "            self.roll  = (1 - self.alpha) * self.roll  + self.alpha * roll\n",
    "        return self.pitch, self.yaw, self.roll\n",
    "\n",
    "kalman_left = TwoDimSmoother()\n",
    "kalman_right = TwoDimSmoother()\n",
    "head_smoother = HeadPoseSmoother()\n",
    "\n",
    "# -------------------- Eye direction --------------------\n",
    "def get_eye_direction(landmarks, eye_idx, iris_idx, w, h, smoother):\n",
    "    eye = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in eye_idx]\n",
    "    iris_pts = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in iris_idx]\n",
    "    iris_cx = sum([p[0] for p in iris_pts]) / len(iris_pts)\n",
    "    iris_cy = sum([p[1] for p in iris_pts]) / len(iris_pts)\n",
    "    iris_x, iris_y = smoother.apply(iris_cx, iris_cy)\n",
    "\n",
    "    left, right = min(eye[0][0], eye[1][0]), max(eye[0][0], eye[1][0])\n",
    "    top, bottom = min(eye[2][1], eye[3][1]), max(eye[2][1], eye[3][1])\n",
    "\n",
    "    margin_x = int((right - left) * 0.15)\n",
    "    margin_y = int((bottom - top) * 0.20)\n",
    "\n",
    "    # Determine gaze direction and offsets\n",
    "    if iris_x < left + margin_x:\n",
    "        return \"LEFT\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    elif iris_x > right - margin_x:\n",
    "        return \"RIGHT\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    elif iris_y < top + margin_y:\n",
    "        return \"UP\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    elif iris_y > bottom - margin_y:\n",
    "        return \"DOWN\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "    else:\n",
    "        return \"CENTER\", iris_x - (left + margin_x), iris_y - (top + margin_y)\n",
    "\n",
    "# -------------------- Head pose --------------------\n",
    "def estimate_head_pose(landmarks, w, h):\n",
    "    image_points = np.array([(landmarks[idx].x * w, landmarks[idx].y * h) for idx in POSE_LANDMARKS], dtype=\"double\")\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),\n",
    "        (-30.0, 0.0, -30.0),\n",
    "        (30.0, 0.0, -30.0),\n",
    "        (-30.0, 0.0, -90.0),\n",
    "        (30.0, 0.0, -90.0),\n",
    "        (0.0, 40.0, -50.0)\n",
    "    ])\n",
    "    focal_length = w\n",
    "    center = (w / 2, h / 2)\n",
    "    camera_matrix = np.array([[focal_length,0,center[0]],[0,focal_length,center[1]],[0,0,1]])\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "    success, rotation_vector, _ = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    return rotation_vector if success else None\n",
    "\n",
    "# -------------------- Collect data --------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "dir_map = {\"LEFT\":0,\"RIGHT\":1,\"UP\":2,\"DOWN\":3,\"CENTER\":4}\n",
    "\n",
    "columns = [\"left_dir\",\"right_dir\",\"left_dx\",\"left_dy\",\"right_dx\",\"right_dy\",\n",
    "           \"pitch\",\"yaw\",\"roll\",\"phone_detected\",\"eyes_away_duration\",\"label\"]\n",
    "data_rows = []\n",
    "\n",
    "away_start_time = None\n",
    "AWAY_THRESHOLD = 0.8\n",
    "\n",
    "print(\"Press 0 for FOCUSED, 1 for DISTRACTED, ESC to quit and train model.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    frame = cv2.flip(frame,1)\n",
    "    h,w,_ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    eyes_away = False\n",
    "    phone_detected = 0\n",
    "\n",
    "    # Face and eyes\n",
    "    face_results = face_mesh.process(rgb)\n",
    "    left_dir = right_dir = \"CENTER\"\n",
    "    left_dx = left_dy = right_dx = right_dy = 0\n",
    "    pitch = yaw = roll = 0\n",
    "    eyes_away_duration = 0\n",
    "\n",
    "    if face_results.multi_face_landmarks:\n",
    "        landmarks = face_results.multi_face_landmarks[0].landmark\n",
    "        left_dir, left_dx, left_dy = get_eye_direction(landmarks, LEFT_EYE, LEFT_IRIS, w, h, kalman_left)\n",
    "        right_dir, right_dx, right_dy = get_eye_direction(landmarks, RIGHT_EYE, RIGHT_IRIS, w, h, kalman_right)\n",
    "        rot_vec = estimate_head_pose(landmarks, w, h)\n",
    "        if rot_vec is not None:\n",
    "            pitch, yaw, roll = head_smoother.apply(*rot_vec.ravel())\n",
    "        if left_dir!=\"CENTER\" or right_dir!=\"CENTER\":\n",
    "            eyes_away = True\n",
    "\n",
    "    # Phone detection\n",
    "    results = phone_model(frame, verbose=False)\n",
    "    for box in results[0].boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        label = phone_model.names[cls]\n",
    "        conf = float(box.conf[0])\n",
    "        if label in PHONE_CLASSES and conf>0.5:\n",
    "            phone_detected = 1\n",
    "\n",
    "    # Eye-away timer\n",
    "    current_time = time.time()\n",
    "    if eyes_away:\n",
    "        if away_start_time is None: away_start_time=current_time\n",
    "        eyes_away_duration = current_time - away_start_time\n",
    "    else:\n",
    "        away_start_time=None\n",
    "        eyes_away_duration=0\n",
    "\n",
    "    # Display frame\n",
    "    cv2.putText(frame,\"Press 0:FOCUSED 1:DISTRACTED\",(10,30),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,255,255),2)\n",
    "    cv2.imshow(\"Data Collection\", frame)\n",
    "\n",
    "    # Wait for key\n",
    "    key = cv2.waitKey(1)\n",
    "    if key==27: break  # ESC to quit\n",
    "    label = None\n",
    "    if key==ord(\"0\"): label=0\n",
    "    elif key==ord(\"1\"): label=1\n",
    "    if label is not None:\n",
    "        row = [dir_map[left_dir], dir_map[right_dir], left_dx, left_dy, right_dx, right_dy,\n",
    "               pitch, yaw, roll, phone_detected, eyes_away_duration, label]\n",
    "        data_rows.append(row)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# -------------------- Train model --------------------\n",
    "df = pd.DataFrame(data_rows, columns=columns)\n",
    "X = df.drop(\"label\", axis=1)\n",
    "y = df[\"label\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X, y)\n",
    "\n",
    "joblib.dump(clf, \"gaze_model.pkl\")\n",
    "print(\"Model trained and saved as gaze_model.pkl!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4de8ef-e96e-4dc3-86c2-801ed4656e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (mp-env)",
   "language": "python",
   "name": "mp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
